{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generierung von Gegenreden zu Verschwörungstheorien mit Mistral-7B-Instruct-v0.3\n",
    "\n",
    "Dieses Notebook demonstriert die Verwendung des Mistral-7B-Instruct-v0.3 Sprachmodells zur Generierung von Gegenreden für einen Datensatz mit 100 verschwörungstheoretischen Kommentaren. Das Skript ist darauf ausgelegt, auf einer GPU ausgeführt zu werden, um die hohen Rechenanforderungen effizient zu bewältigen. Durch den Einsatz des Mistral-Modells wird jeder Kommentar verarbeitet, um eine prägnante und effektive Gegenrede zu erzeugen, die spezifischen Richtlinien folgt, wie etwa Empathie, Faktentreue und Klarheit. Der resultierende Datensatz enthält sowohl die ursprünglichen Verschwörungstheorie-Kommentare als auch die generierten Gegenreden, was wertvolle Einblicke für weiterführende Analysen und Studien bietet.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed76fd40e4d9fe23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load dataset with conspiracy theory comments\n",
    "ct_dataset = pd.read_csv('../data/qanon_deepstate_comments.csv')\n",
    "\n",
    "# Initialize model and tokenizer \n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Function to generate counter speech\n",
    "def generate_counter_speech(comment):\n",
    "    \n",
    "    # Defining the system and user messages\n",
    "    system_prompt = \"\"\"You are a trained expert in generating counter speech to conspiracy theory comments.\n",
    "             Follow these response guidelines: \n",
    "                1. Show empathy and positivity in your response.\n",
    "                2. Do not state 'this is a conspiracy theory' directly.\n",
    "                3. Use narrative storytelling, including a first-person perspective, detailed accounts of characters' internal lives, metaphors and figurative language.  Include a relatable protagonist (well-known figures only) or credible real-life examples to illustrate your point. \n",
    "                4. Ensure clarity in your argumentation with defined objectives.\n",
    "                5. Challenge the statement and refute it with specific facts from reliable sources. If appropriate, ask for sources or factual basis.\n",
    "                6. Maintain a respectful and calm tone throughout your response. Be cautious with sarcasm, humor, parody, and satire.\n",
    "                7. Always respond concisely, directly, and clearly. Limit your response to 800 characters. \n",
    "                \"\"\"\n",
    "    user_prompt = f\"Generate counter speech to the following conspiracy theory comment: {comment}.\"\n",
    "    prefix = \"Very concise and short counter speech that uses less than 200 tokens:\"\n",
    "    \n",
    "    # Formats the messages according to Mistral's requirements\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{system_prompt} {user_prompt}\"},\n",
    "        {\"role\": \"assistant\", \"content\": prefix, \"prefix\": True}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = inputs.to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs, \n",
    "        max_new_tokens=350, \n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    outputs = generated_ids[:, model_inputs.shape[-1]:]  # Cut off the original input length\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return response[0]\n",
    "    \n",
    "# Generate counter speech for every comment and save in a new column\n",
    "ct_dataset['counter_speech_mistral'] = ct_dataset['comment_text'].apply(generate_counter_speech)\n",
    "\n",
    "# Save updated dataset\n",
    "ct_dataset.to_csv('../data/counterspeech_dataset_mistral.csv', index=False)\n",
    "\n",
    "# Clean up resources to prevent memory leaks\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e97ced7137f2a62c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
