{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generierung von Gegenreden zu Verschwörungstheorien mit Meta-Llama-3-8B-Instruct\n",
    "\n",
    "Dieses Notebook zeigt die Verwendung des vortrainierten Sprachmodells Meta-Llama-3-8B-Instruct zur Generierung von Gegenreden für einen Datensatz mit 100 verschwörungstheoretischen Kommentaren. Das Skript ist für die Ausführung auf einer GPU optimiert, um die hohen Rechenanforderungen effizient zu bewältigen und die Generierung von Gegenreden reibungslos zu ermöglichen. Die erzeugten Antworten folgen spezifischen Richtlinien, die besonders Wert auf Empathie, Klarheit und den richtigen Ton legen. Das Endergebnis ist ein neuer Datensatz, der sowohl die ursprünglichen Verschwörungstheorie-Kommentare als auch die entsprechenden Gegenreden enthält und für weiterführende Analysen oder Trainingszwecke genutzt werden kann."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343118f9f842452c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load dataset with conspiracy theory comments\n",
    "ct_dataset = pd.read_csv('../data/qanon_deepstate_comments.csv')\n",
    "\n",
    "# Initialize model and tokenizer \n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n",
    "\n",
    "# Function to generate counter speech\n",
    "def generate_counter_speech(comment):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"\"\"You are a trained expert in generating counter speech to conspiracy theory comments. \n",
    "                        Follow these response guidelines: \n",
    "                        1. Show empathy and positivity in your response.\n",
    "                        2. Do not state 'this is a conspiracy theory' directly.\n",
    "                        3. Use narrative storytelling, including a first-person perspective, detailed accounts of characters' internal lives, metaphors and figurative language.  Include a relatable protagonist (well-known figures only) or credible real-life examples to illustrate your point.\n",
    "                        4. Ensure clarity in your argumentation with defined objectives.\n",
    "                        5. Challenge the statement and refute it with specific facts from reliable sources. If appropriate, ask for sources or factual basis.\n",
    "                        6. Maintain a respectful and calm tone throughout your response. Be cautious with sarcasm, humor, parody, and satire.\n",
    "                        7. Always respond concisely, directly, and clearly. Limit your response to 800 characters.\n",
    "                        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate counter speech to the following conspiracy theory comment: {comment}\"}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=350,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# Generate counter speech for every comment and save in appropriate column\n",
    "ct_dataset['counter_speech_llama3'] = ct_dataset['comment_text'].apply(generate_counter_speech)\n",
    "\n",
    "# Save results in a new file\n",
    "ct_dataset.to_csv('../data/counterspeech_dataset_llama3.csv', index=False)\n",
    "\n",
    "# Clean up resources to prevent memory leaks\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
