{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generating Counter Speech Responses to Conspiracy Theory Comments Using Meta-Llama-3-8B-Instruct\n",
    "\n",
    "This notebook demonstrates the use of a pretrained language model, Meta-Llama-3-8B-Instruct, to generate counter speech responses for a dataset of 100 conspiracy theory comments. The script is designed to run on a GPU to efficiently handle the computational demands, allowing the model to generate counter speech efficiently. The generated responses adhere to specific guidelines, emphasizing empathy, clarity and tone. The final output is a new dataset that contains both the original conspiracy comments and the corresponding counter speech responses, which can be used for further analysis or training.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343118f9f842452c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "ct_dataset = pd.read_csv('../data/qanon_deepstate_comments.csv', delimiter=';', quotechar='\"')\n",
    "\n",
    "# Initialize model and tokenizer \n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n",
    "\n",
    "# Function to generate counter speech\n",
    "def generate_counter_speech(comment):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"\"\"You are a trained expert in generating counter speech to conspiracy theory comments. \n",
    "                        Follow these response guidelines: \n",
    "                        1. Show empathy and positivity in your response.\n",
    "                        2. Do not state 'this is a conspiracy theory' directly.\n",
    "                        3. Use narrative storytelling, including a first-person perspective, detailed accounts of characters' internal lives, metaphors and figurative language.  Include a relatable protagonist (well-known figures only) or credible real-life examples to illustrate your point.\n",
    "                        4. Ensure clarity in your argumentation with defined objectives.\n",
    "                        5. Challenge the statement and refute it with specific facts from reliable sources. If appropriate, ask for sources or factual basis.\n",
    "                        6. Maintain a respectful and calm tone throughout your response. Be cautious with sarcasm, humor, parody, and satire.\n",
    "                        7. Always respond concisely, directly, and clearly. Limit your response to 800 characters.\n",
    "                        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Generate counter speech to the following conspiracy theory comment: {comment}\"}\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=350,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "# Generate counter speech for every comment and save in appropriate column\n",
    "ct_dataset['counter_speech_llama3'] = ct_dataset['comment_text'].apply(generate_counter_speech)\n",
    "\n",
    "# Save results in a new file\n",
    "ct_dataset.to_csv('../data/counterspeech_dataset_llama3.csv', index=False)\n",
    "\n",
    "# Clean up resources to prevent memory leaks\n",
    "del model, tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
